{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = 'Genre Classification Dataset'\n",
    "train_file = 'train_data.txt'\n",
    "test_file = 'test_data_solution.txt'\n",
    "\n",
    "def get_dataframe(file_name):\n",
    "    data = []\n",
    "\n",
    "    with open(os.path.join(folder, file_name)) as f:\n",
    "\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            \n",
    "            row = line.split(' ::: ')\n",
    "\n",
    "            try:\n",
    "                title = row[1].split(' (')[0]\n",
    "                year = row[1].split(' (')[1].split(')')[0]\n",
    "                genre = row[2]\n",
    "                description = row[3]\n",
    "                data.append([title, year, genre, description])\n",
    "            except Exception as exp:\n",
    "                print(exp)\n",
    "\n",
    "    return pd.DataFrame(data, columns=['title', 'year', 'genre', 'description'])\n",
    "\n",
    "df_train = get_dataframe(train_file)\n",
    "df_test = get_dataframe(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 54214, test dataset: 54200, unique labels: 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oscar et la dame rose</td>\n",
       "      <td>2009</td>\n",
       "      <td>drama</td>\n",
       "      <td>Listening in to a conversation between his doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cupid</td>\n",
       "      <td>1997</td>\n",
       "      <td>thriller</td>\n",
       "      <td>A brother and sister with a past incestuous re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Young, Wild and Wonderful</td>\n",
       "      <td>1980</td>\n",
       "      <td>adult</td>\n",
       "      <td>As the bus empties the students for their fiel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Secret Sin</td>\n",
       "      <td>1915</td>\n",
       "      <td>drama</td>\n",
       "      <td>To help their unemployed father make ends meet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Unrecovered</td>\n",
       "      <td>2007</td>\n",
       "      <td>drama</td>\n",
       "      <td>The film's title refers not only to the un-rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  year     genre  \\\n",
       "0      Oscar et la dame rose  2009     drama   \n",
       "1                      Cupid  1997  thriller   \n",
       "2  Young, Wild and Wonderful  1980     adult   \n",
       "3             The Secret Sin  1915     drama   \n",
       "4            The Unrecovered  2007     drama   \n",
       "\n",
       "                                         description  \n",
       "0  Listening in to a conversation between his doc...  \n",
       "1  A brother and sister with a past incestuous re...  \n",
       "2  As the bus empties the students for their fiel...  \n",
       "3  To help their unemployed father make ends meet...  \n",
       "4  The film's title refers not only to the un-rec...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Size of training dataset: {len(df_train)}, test dataset: {len(df_test)}, unique labels: {len(np.unique(df_train['genre']))}\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edgar's Lunch</td>\n",
       "      <td>1998</td>\n",
       "      <td>thriller</td>\n",
       "      <td>L.R. Brane loves his life - his car, his apart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La guerra de papá</td>\n",
       "      <td>1977</td>\n",
       "      <td>comedy</td>\n",
       "      <td>Spain, March 1964: Quico is a very naughty chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off the Beaten Track</td>\n",
       "      <td>2010</td>\n",
       "      <td>documentary</td>\n",
       "      <td>One year in the life of Albin and his family o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meu Amigo Hindu</td>\n",
       "      <td>2015</td>\n",
       "      <td>drama</td>\n",
       "      <td>His father has died, he hasn't spoken with his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Er nu zhai</td>\n",
       "      <td>1955</td>\n",
       "      <td>drama</td>\n",
       "      <td>Before he was known internationally as a marti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  year        genre  \\\n",
       "0         Edgar's Lunch  1998     thriller   \n",
       "1     La guerra de papá  1977       comedy   \n",
       "2  Off the Beaten Track  2010  documentary   \n",
       "3       Meu Amigo Hindu  2015        drama   \n",
       "4            Er nu zhai  1955        drama   \n",
       "\n",
       "                                         description  \n",
       "0  L.R. Brane loves his life - his car, his apart...  \n",
       "1  Spain, March 1964: Quico is a very naughty chi...  \n",
       "2  One year in the life of Albin and his family o...  \n",
       "3  His father has died, he hasn't spoken with his...  \n",
       "4  Before he was known internationally as a marti...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['title', 'year', 'description']\n",
    "output_features = ['genre']\n",
    "\n",
    "X_train = df_train[input_features]\n",
    "y_train = df_train[output_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for the model classifying genres based on title, year, description.\n",
    "\n",
    "1. Vectorize the data:\n",
    "    - genre2vec (straightforward)\n",
    "    - title2vec (complex)\n",
    "    - decription2vec (complex)\n",
    "2. Build 2 models and unittest them:\n",
    "    - ready-to-use model from a popular library\n",
    "    - implement LSTM with PyTorch\n",
    "    - implement a model using transformers ^^\n",
    "\n",
    "3. Train & debug models:\n",
    "    - ready-to-use: 0-small # of bugs expected\n",
    "    - LSTM: moderate # of bugs expected\n",
    "    - Transformers: high # of bugs expected\n",
    "\n",
    "4. Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27,) ['action' 'adult' 'adventure' 'animation' 'biography' 'comedy' 'crime'\n",
      " 'documentary' 'drama' 'family' 'fantasy' 'game-show' 'history' 'horror'\n",
      " 'music' 'musical' 'mystery' 'news' 'reality-tv' 'romance' 'sci-fi'\n",
      " 'short' 'sport' 'talk-show' 'thriller' 'war' 'western']\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_categories(dataframe, target):\n",
    "    return dataframe.join(pd.get_dummies(dataframe[target])).drop(columns=target)\n",
    "\n",
    "num_df_train = one_hot_encode_categories(df_train, 'genre')\n",
    "num_df_test = one_hot_encode_categories(df_test, 'genre')\n",
    "\n",
    "output_features_one_hot = np.unique(df_train[output_features])\n",
    "print(output_features_one_hot.shape, output_features_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "      <th>action</th>\n",
       "      <th>adult</th>\n",
       "      <th>adventure</th>\n",
       "      <th>animation</th>\n",
       "      <th>biography</th>\n",
       "      <th>comedy</th>\n",
       "      <th>crime</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>reality-tv</th>\n",
       "      <th>romance</th>\n",
       "      <th>sci-fi</th>\n",
       "      <th>short</th>\n",
       "      <th>sport</th>\n",
       "      <th>talk-show</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oscar et la dame rose</td>\n",
       "      <td>2009</td>\n",
       "      <td>Listening in to a conversation between his doc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cupid</td>\n",
       "      <td>1997</td>\n",
       "      <td>A brother and sister with a past incestuous re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Young, Wild and Wonderful</td>\n",
       "      <td>1980</td>\n",
       "      <td>As the bus empties the students for their fiel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Secret Sin</td>\n",
       "      <td>1915</td>\n",
       "      <td>To help their unemployed father make ends meet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Unrecovered</td>\n",
       "      <td>2007</td>\n",
       "      <td>The film's title refers not only to the un-rec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  year  \\\n",
       "0      Oscar et la dame rose  2009   \n",
       "1                      Cupid  1997   \n",
       "2  Young, Wild and Wonderful  1980   \n",
       "3             The Secret Sin  1915   \n",
       "4            The Unrecovered  2007   \n",
       "\n",
       "                                         description  action  adult  \\\n",
       "0  Listening in to a conversation between his doc...       0      0   \n",
       "1  A brother and sister with a past incestuous re...       0      0   \n",
       "2  As the bus empties the students for their fiel...       0      1   \n",
       "3  To help their unemployed father make ends meet...       0      0   \n",
       "4  The film's title refers not only to the un-rec...       0      0   \n",
       "\n",
       "   adventure  animation  biography  comedy  crime  ...  news  reality-tv  \\\n",
       "0          0          0          0       0      0  ...     0           0   \n",
       "1          0          0          0       0      0  ...     0           0   \n",
       "2          0          0          0       0      0  ...     0           0   \n",
       "3          0          0          0       0      0  ...     0           0   \n",
       "4          0          0          0       0      0  ...     0           0   \n",
       "\n",
       "   romance  sci-fi  short  sport  talk-show  thriller  war  western  \n",
       "0        0       0      0      0          0         0    0        0  \n",
       "1        0       0      0      0          0         1    0        0  \n",
       "2        0       0      0      0          0         0    0        0  \n",
       "3        0       0      0      0          0         0    0        0  \n",
       "4        0       0      0      0          0         0    0        0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "      <th>action</th>\n",
       "      <th>adult</th>\n",
       "      <th>adventure</th>\n",
       "      <th>animation</th>\n",
       "      <th>biography</th>\n",
       "      <th>comedy</th>\n",
       "      <th>crime</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>reality-tv</th>\n",
       "      <th>romance</th>\n",
       "      <th>sci-fi</th>\n",
       "      <th>short</th>\n",
       "      <th>sport</th>\n",
       "      <th>talk-show</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edgar's Lunch</td>\n",
       "      <td>1998</td>\n",
       "      <td>L.R. Brane loves his life - his car, his apart...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La guerra de papá</td>\n",
       "      <td>1977</td>\n",
       "      <td>Spain, March 1964: Quico is a very naughty chi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off the Beaten Track</td>\n",
       "      <td>2010</td>\n",
       "      <td>One year in the life of Albin and his family o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meu Amigo Hindu</td>\n",
       "      <td>2015</td>\n",
       "      <td>His father has died, he hasn't spoken with his...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Er nu zhai</td>\n",
       "      <td>1955</td>\n",
       "      <td>Before he was known internationally as a marti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  year  \\\n",
       "0         Edgar's Lunch  1998   \n",
       "1     La guerra de papá  1977   \n",
       "2  Off the Beaten Track  2010   \n",
       "3       Meu Amigo Hindu  2015   \n",
       "4            Er nu zhai  1955   \n",
       "\n",
       "                                         description  action  adult  \\\n",
       "0  L.R. Brane loves his life - his car, his apart...       0      0   \n",
       "1  Spain, March 1964: Quico is a very naughty chi...       0      0   \n",
       "2  One year in the life of Albin and his family o...       0      0   \n",
       "3  His father has died, he hasn't spoken with his...       0      0   \n",
       "4  Before he was known internationally as a marti...       0      0   \n",
       "\n",
       "   adventure  animation  biography  comedy  crime  ...  news  reality-tv  \\\n",
       "0          0          0          0       0      0  ...     0           0   \n",
       "1          0          0          0       1      0  ...     0           0   \n",
       "2          0          0          0       0      0  ...     0           0   \n",
       "3          0          0          0       0      0  ...     0           0   \n",
       "4          0          0          0       0      0  ...     0           0   \n",
       "\n",
       "   romance  sci-fi  short  sport  talk-show  thriller  war  western  \n",
       "0        0       0      0      0          0         1    0        0  \n",
       "1        0       0      0      0          0         0    0        0  \n",
       "2        0       0      0      0          0         0    0        0  \n",
       "3        0       0      0      0          0         0    0        0  \n",
       "4        0       0      0      0          0         0    0        0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# If not working python run python3 -m spacy download en_core_web_sm\n",
    "from nltk import sent_tokenize\n",
    "# If not working run nltk.download()\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize(description):\n",
    "    sentences = sent_tokenize(description)\n",
    "    description_lem = [word.lemma_ for sentence in sentences for word in nlp(sentence)]\n",
    "    return description_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "description_train_lem = num_df_train[0:k].apply(lambda x: lemmatize(x['description']), axis = 1)\n",
    "description_test_lem = num_df_test[0:k].apply(lambda x: lemmatize(x['description']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_train_lem = num_df_train[0:k].apply(lambda x: lemmatize(x['title']), axis = 1)\n",
    "title_test_lem = num_df_test[0:k].apply(lambda x: lemmatize(x['title']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "      <th>action</th>\n",
       "      <th>adult</th>\n",
       "      <th>adventure</th>\n",
       "      <th>animation</th>\n",
       "      <th>biography</th>\n",
       "      <th>comedy</th>\n",
       "      <th>crime</th>\n",
       "      <th>...</th>\n",
       "      <th>romance</th>\n",
       "      <th>sci-fi</th>\n",
       "      <th>short</th>\n",
       "      <th>sport</th>\n",
       "      <th>talk-show</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "      <th>description_lem</th>\n",
       "      <th>title_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oscar et la dame rose</td>\n",
       "      <td>2009</td>\n",
       "      <td>Listening in to a conversation between his doc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[listen, in, to, a, conversation, between, his...</td>\n",
       "      <td>[Oscar, et, la, dame, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cupid</td>\n",
       "      <td>1997</td>\n",
       "      <td>A brother and sister with a past incestuous re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[a, brother, and, sister, with, a, past, inces...</td>\n",
       "      <td>[cupid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Young, Wild and Wonderful</td>\n",
       "      <td>1980</td>\n",
       "      <td>As the bus empties the students for their fiel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[as, the, bus, empty, the, student, for, their...</td>\n",
       "      <td>[young, ,, wild, and, wonderful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Secret Sin</td>\n",
       "      <td>1915</td>\n",
       "      <td>To help their unemployed father make ends meet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, help, their, unemployed, father, make, en...</td>\n",
       "      <td>[the, Secret, sin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Unrecovered</td>\n",
       "      <td>2007</td>\n",
       "      <td>The film's title refers not only to the un-rec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, film, 's, title, refer, not, only, to, t...</td>\n",
       "      <td>[the, Unrecovered]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Dangerous Orphans</td>\n",
       "      <td>1985</td>\n",
       "      <td>Harry, Moir and Rossi were like the three musk...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Harry, ,, Moir, and, Rossi, be, like, the, th...</td>\n",
       "      <td>[dangerous, Orphans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Counting Backwards</td>\n",
       "      <td>2007</td>\n",
       "      <td>For some, the lives we have are not always the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[for, some, ,, the, life, we, have, be, not, a...</td>\n",
       "      <td>[count, backwards]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Bubble Gum</td>\n",
       "      <td>2011/II</td>\n",
       "      <td>Jamshedpur-based Vedant Rawat lives a middle-c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Jamshedpur, -, base, Vedant, Rawat, live, a, ...</td>\n",
       "      <td>[bubble, gum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The Hammer of Mara: Mephisto's Wrath</td>\n",
       "      <td>2015</td>\n",
       "      <td>Following the events of The Hammer of Mara, Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[follow, the, event, of, the, Hammer, of, Mara...</td>\n",
       "      <td>[the, Hammer, of, Mara, :, Mephisto, 's, wrath]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Pomroy's People</td>\n",
       "      <td>1973</td>\n",
       "      <td>When the teacher of school kids the small town...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[when, the, teacher, of, school, kid, the, sma...</td>\n",
       "      <td>[Pomroy, 's, People]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title     year  \\\n",
       "0                   Oscar et la dame rose     2009   \n",
       "1                                   Cupid     1997   \n",
       "2               Young, Wild and Wonderful     1980   \n",
       "3                          The Secret Sin     1915   \n",
       "4                         The Unrecovered     2007   \n",
       "..                                    ...      ...   \n",
       "995                     Dangerous Orphans     1985   \n",
       "996                    Counting Backwards     2007   \n",
       "997                            Bubble Gum  2011/II   \n",
       "998  The Hammer of Mara: Mephisto's Wrath     2015   \n",
       "999                       Pomroy's People     1973   \n",
       "\n",
       "                                           description  action  adult  \\\n",
       "0    Listening in to a conversation between his doc...       0      0   \n",
       "1    A brother and sister with a past incestuous re...       0      0   \n",
       "2    As the bus empties the students for their fiel...       0      1   \n",
       "3    To help their unemployed father make ends meet...       0      0   \n",
       "4    The film's title refers not only to the un-rec...       0      0   \n",
       "..                                                 ...     ...    ...   \n",
       "995  Harry, Moir and Rossi were like the three musk...       1      0   \n",
       "996  For some, the lives we have are not always the...       0      0   \n",
       "997  Jamshedpur-based Vedant Rawat lives a middle-c...       0      0   \n",
       "998  Following the events of The Hammer of Mara, Ma...       1      0   \n",
       "999  When the teacher of school kids the small town...       0      0   \n",
       "\n",
       "     adventure  animation  biography  comedy  crime  ...  romance  sci-fi  \\\n",
       "0            0          0          0       0      0  ...        0       0   \n",
       "1            0          0          0       0      0  ...        0       0   \n",
       "2            0          0          0       0      0  ...        0       0   \n",
       "3            0          0          0       0      0  ...        0       0   \n",
       "4            0          0          0       0      0  ...        0       0   \n",
       "..         ...        ...        ...     ...    ...  ...      ...     ...   \n",
       "995          0          0          0       0      0  ...        0       0   \n",
       "996          0          0          0       0      0  ...        1       0   \n",
       "997          0          0          0       0      0  ...        0       0   \n",
       "998          0          0          0       0      0  ...        0       0   \n",
       "999          0          0          0       0      0  ...        0       0   \n",
       "\n",
       "     short  sport  talk-show  thriller  war  western  \\\n",
       "0        0      0          0         0    0        0   \n",
       "1        0      0          0         1    0        0   \n",
       "2        0      0          0         0    0        0   \n",
       "3        0      0          0         0    0        0   \n",
       "4        0      0          0         0    0        0   \n",
       "..     ...    ...        ...       ...  ...      ...   \n",
       "995      0      0          0         0    0        0   \n",
       "996      0      0          0         0    0        0   \n",
       "997      0      0          0         0    0        0   \n",
       "998      0      0          0         0    0        0   \n",
       "999      0      0          0         0    0        0   \n",
       "\n",
       "                                       description_lem  \\\n",
       "0    [listen, in, to, a, conversation, between, his...   \n",
       "1    [a, brother, and, sister, with, a, past, inces...   \n",
       "2    [as, the, bus, empty, the, student, for, their...   \n",
       "3    [to, help, their, unemployed, father, make, en...   \n",
       "4    [the, film, 's, title, refer, not, only, to, t...   \n",
       "..                                                 ...   \n",
       "995  [Harry, ,, Moir, and, Rossi, be, like, the, th...   \n",
       "996  [for, some, ,, the, life, we, have, be, not, a...   \n",
       "997  [Jamshedpur, -, base, Vedant, Rawat, live, a, ...   \n",
       "998  [follow, the, event, of, the, Hammer, of, Mara...   \n",
       "999  [when, the, teacher, of, school, kid, the, sma...   \n",
       "\n",
       "                                           title_lem  \n",
       "0                        [Oscar, et, la, dame, rise]  \n",
       "1                                            [cupid]  \n",
       "2                   [young, ,, wild, and, wonderful]  \n",
       "3                                 [the, Secret, sin]  \n",
       "4                                 [the, Unrecovered]  \n",
       "..                                               ...  \n",
       "995                             [dangerous, Orphans]  \n",
       "996                               [count, backwards]  \n",
       "997                                    [bubble, gum]  \n",
       "998  [the, Hammer, of, Mara, :, Mephisto, 's, wrath]  \n",
       "999                             [Pomroy, 's, People]  \n",
       "\n",
       "[1000 rows x 32 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preproc = num_df_train[:k].join(pd.DataFrame({'description_lem': description_train_lem})).join(pd.DataFrame({'title_lem': title_train_lem}))\n",
    "df_test_preproc = num_df_test[:k].join(pd.DataFrame({'description_lem': description_test_lem})).join(pd.DataFrame({'title_lem': title_test_lem}))\n",
    "\n",
    "input_features_preproc = ['description_lem', 'title', 'year']\n",
    "output_features_preproc = output_features_one_hot\n",
    "\n",
    "df_train_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 13:40:33,470 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>', 'datetime': '2023-01-06T13:40:33.469925', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'created'}\n",
      "2023-01-06 13:40:33,471 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d10,n5,w5,s0.001,t3>', 'datetime': '2023-01-06T13:40:33.471323', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "documents_train_desc = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_train_preproc['description_lem'])]\n",
    "documents_test_desc = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_test_preproc['description_lem'])]\n",
    "documents_train_title = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_train_preproc['title_lem'])]\n",
    "documents_test_title = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_test_preproc['title_lem'])]\n",
    "\n",
    "model_desc = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model_title = Doc2Vec(vector_size=10, min_count=1, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 13:40:52,919 : INFO : collecting all words and their counts\n",
      "2023-01-06 13:40:52,921 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-01-06 13:40:52,934 : INFO : collected 13359 word types and 1000 unique tags from a corpus of 1000 examples and 120217 words\n",
      "2023-01-06 13:40:52,935 : INFO : Creating a fresh vocabulary\n",
      "2023-01-06 13:40:52,946 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 6326 unique words (47.35% of original 13359, drops 7033)', 'datetime': '2023-01-06T13:40:52.946904', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-01-06 13:40:52,947 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 113184 word corpus (94.15% of original 120217, drops 7033)', 'datetime': '2023-01-06T13:40:52.947806', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-01-06 13:40:52,961 : INFO : deleting the raw counts dictionary of 13359 items\n",
      "2023-01-06 13:40:52,962 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2023-01-06 13:40:52,962 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 77280.52196717598 word corpus (68.3%% of prior 113184)', 'datetime': '2023-01-06T13:40:52.962457', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-01-06 13:40:52,987 : INFO : estimated required memory for 6326 words and 50 dimensions: 6093400 bytes\n",
      "2023-01-06 13:40:52,987 : INFO : resetting layer weights\n",
      "2023-01-06 13:40:52,990 : INFO : collecting all words and their counts\n",
      "2023-01-06 13:40:52,991 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-01-06 13:40:52,992 : INFO : collected 2165 word types and 1000 unique tags from a corpus of 1000 examples and 3749 words\n",
      "2023-01-06 13:40:52,992 : INFO : Creating a fresh vocabulary\n",
      "2023-01-06 13:40:52,996 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2165 unique words (100.00% of original 2165, drops 0)', 'datetime': '2023-01-06T13:40:52.996303', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-01-06 13:40:52,997 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3749 word corpus (100.00% of original 3749, drops 0)', 'datetime': '2023-01-06T13:40:52.997068', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-01-06 13:40:53,002 : INFO : deleting the raw counts dictionary of 2165 items\n",
      "2023-01-06 13:40:53,003 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2023-01-06 13:40:53,003 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3014.710969407154 word corpus (80.4%% of prior 3749)', 'datetime': '2023-01-06T13:40:53.003742', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-01-06 13:40:53,011 : INFO : estimated required memory for 2165 words and 10 dimensions: 1495700 bytes\n",
      "2023-01-06 13:40:53,012 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model_desc.build_vocab(documents_train_desc)\n",
    "model_title.build_vocab(documents_train_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'year' appeared 257 times in the training corpus.\n",
      "Word 'year' appeared 1 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'year' appeared {model_desc.wv.get_vecattr('year', 'count')} times in the training corpus.\")\n",
    "print(f\"Word 'year' appeared {model_title.wv.get_vecattr('year', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 13:41:27,231 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 6326 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-01-06T13:41:27.231189', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-01-06 13:41:27,291 : INFO : EPOCH 0: training on 120217 raw words (78136 effective words) took 0.1s, 1332512 effective words/s\n",
      "2023-01-06 13:41:27,343 : INFO : EPOCH 1: training on 120217 raw words (78307 effective words) took 0.1s, 1530861 effective words/s\n",
      "2023-01-06 13:41:27,397 : INFO : EPOCH 2: training on 120217 raw words (78209 effective words) took 0.1s, 1505230 effective words/s\n",
      "2023-01-06 13:41:27,449 : INFO : EPOCH 3: training on 120217 raw words (78367 effective words) took 0.1s, 1537972 effective words/s\n",
      "2023-01-06 13:41:27,502 : INFO : EPOCH 4: training on 120217 raw words (78398 effective words) took 0.1s, 1516181 effective words/s\n",
      "2023-01-06 13:41:27,552 : INFO : EPOCH 5: training on 120217 raw words (78321 effective words) took 0.0s, 1594252 effective words/s\n",
      "2023-01-06 13:41:27,604 : INFO : EPOCH 6: training on 120217 raw words (78341 effective words) took 0.1s, 1557673 effective words/s\n",
      "2023-01-06 13:41:27,655 : INFO : EPOCH 7: training on 120217 raw words (78290 effective words) took 0.0s, 1566090 effective words/s\n",
      "2023-01-06 13:41:27,706 : INFO : EPOCH 8: training on 120217 raw words (78094 effective words) took 0.0s, 1581380 effective words/s\n",
      "2023-01-06 13:41:27,764 : INFO : EPOCH 9: training on 120217 raw words (78312 effective words) took 0.1s, 1391647 effective words/s\n",
      "2023-01-06 13:41:27,816 : INFO : EPOCH 10: training on 120217 raw words (78385 effective words) took 0.1s, 1523202 effective words/s\n",
      "2023-01-06 13:41:27,868 : INFO : EPOCH 11: training on 120217 raw words (78367 effective words) took 0.1s, 1550463 effective words/s\n",
      "2023-01-06 13:41:27,919 : INFO : EPOCH 12: training on 120217 raw words (78271 effective words) took 0.0s, 1579953 effective words/s\n",
      "2023-01-06 13:41:27,970 : INFO : EPOCH 13: training on 120217 raw words (78297 effective words) took 0.0s, 1574487 effective words/s\n",
      "2023-01-06 13:41:28,022 : INFO : EPOCH 14: training on 120217 raw words (78263 effective words) took 0.1s, 1521403 effective words/s\n",
      "2023-01-06 13:41:28,074 : INFO : EPOCH 15: training on 120217 raw words (78294 effective words) took 0.1s, 1560801 effective words/s\n",
      "2023-01-06 13:41:28,126 : INFO : EPOCH 16: training on 120217 raw words (78429 effective words) took 0.1s, 1519939 effective words/s\n",
      "2023-01-06 13:41:28,179 : INFO : EPOCH 17: training on 120217 raw words (78443 effective words) took 0.1s, 1532039 effective words/s\n",
      "2023-01-06 13:41:28,231 : INFO : EPOCH 18: training on 120217 raw words (78071 effective words) took 0.1s, 1521240 effective words/s\n",
      "2023-01-06 13:41:28,283 : INFO : EPOCH 19: training on 120217 raw words (78194 effective words) took 0.1s, 1549189 effective words/s\n",
      "2023-01-06 13:41:28,337 : INFO : EPOCH 20: training on 120217 raw words (78389 effective words) took 0.1s, 1489256 effective words/s\n",
      "2023-01-06 13:41:28,390 : INFO : EPOCH 21: training on 120217 raw words (78372 effective words) took 0.1s, 1521274 effective words/s\n",
      "2023-01-06 13:41:28,442 : INFO : EPOCH 22: training on 120217 raw words (78284 effective words) took 0.1s, 1541379 effective words/s\n",
      "2023-01-06 13:41:28,497 : INFO : EPOCH 23: training on 120217 raw words (78378 effective words) took 0.1s, 1474499 effective words/s\n",
      "2023-01-06 13:41:28,550 : INFO : EPOCH 24: training on 120217 raw words (78339 effective words) took 0.1s, 1520443 effective words/s\n",
      "2023-01-06 13:41:28,601 : INFO : EPOCH 25: training on 120217 raw words (78245 effective words) took 0.0s, 1580485 effective words/s\n",
      "2023-01-06 13:41:28,656 : INFO : EPOCH 26: training on 120217 raw words (78334 effective words) took 0.1s, 1445008 effective words/s\n",
      "2023-01-06 13:41:28,708 : INFO : EPOCH 27: training on 120217 raw words (78340 effective words) took 0.1s, 1551886 effective words/s\n",
      "2023-01-06 13:41:28,760 : INFO : EPOCH 28: training on 120217 raw words (78277 effective words) took 0.1s, 1519053 effective words/s\n",
      "2023-01-06 13:41:28,811 : INFO : EPOCH 29: training on 120217 raw words (78270 effective words) took 0.0s, 1593564 effective words/s\n",
      "2023-01-06 13:41:28,862 : INFO : EPOCH 30: training on 120217 raw words (78325 effective words) took 0.1s, 1558542 effective words/s\n",
      "2023-01-06 13:41:28,912 : INFO : EPOCH 31: training on 120217 raw words (78479 effective words) took 0.0s, 1636768 effective words/s\n",
      "2023-01-06 13:41:28,963 : INFO : EPOCH 32: training on 120217 raw words (78212 effective words) took 0.0s, 1568468 effective words/s\n",
      "2023-01-06 13:41:29,009 : INFO : EPOCH 33: training on 120217 raw words (78162 effective words) took 0.0s, 1719813 effective words/s\n",
      "2023-01-06 13:41:29,061 : INFO : EPOCH 34: training on 120217 raw words (78194 effective words) took 0.1s, 1557614 effective words/s\n",
      "2023-01-06 13:41:29,111 : INFO : EPOCH 35: training on 120217 raw words (78287 effective words) took 0.0s, 1611110 effective words/s\n",
      "2023-01-06 13:41:29,161 : INFO : EPOCH 36: training on 120217 raw words (78394 effective words) took 0.0s, 1595342 effective words/s\n",
      "2023-01-06 13:41:29,212 : INFO : EPOCH 37: training on 120217 raw words (78533 effective words) took 0.0s, 1602506 effective words/s\n",
      "2023-01-06 13:41:29,263 : INFO : EPOCH 38: training on 120217 raw words (78155 effective words) took 0.1s, 1553115 effective words/s\n",
      "2023-01-06 13:41:29,312 : INFO : EPOCH 39: training on 120217 raw words (78288 effective words) took 0.0s, 1621625 effective words/s\n",
      "2023-01-06 13:41:29,313 : INFO : Doc2Vec lifecycle event {'msg': 'training on 4808680 raw words (3132046 effective words) took 2.1s, 1504598 effective words/s', 'datetime': '2023-01-06T13:41:29.313320', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "model_desc.train(documents_train_desc, total_examples=model_desc.corpus_count, epochs=model_desc.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 13:41:45,853 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 2165 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-01-06T13:41:45.853497', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-01-06 13:41:45,869 : INFO : EPOCH 0: training on 3749 raw words (4015 effective words) took 0.0s, 308351 effective words/s\n",
      "2023-01-06 13:41:45,885 : INFO : EPOCH 1: training on 3749 raw words (4030 effective words) took 0.0s, 372928 effective words/s\n",
      "2023-01-06 13:41:45,902 : INFO : EPOCH 2: training on 3749 raw words (3987 effective words) took 0.0s, 339040 effective words/s\n",
      "2023-01-06 13:41:45,915 : INFO : EPOCH 3: training on 3749 raw words (4011 effective words) took 0.0s, 373212 effective words/s\n",
      "2023-01-06 13:41:45,927 : INFO : EPOCH 4: training on 3749 raw words (4025 effective words) took 0.0s, 379934 effective words/s\n",
      "2023-01-06 13:41:45,939 : INFO : EPOCH 5: training on 3749 raw words (4005 effective words) took 0.0s, 364159 effective words/s\n",
      "2023-01-06 13:41:45,952 : INFO : EPOCH 6: training on 3749 raw words (4005 effective words) took 0.0s, 358065 effective words/s\n",
      "2023-01-06 13:41:45,964 : INFO : EPOCH 7: training on 3749 raw words (4003 effective words) took 0.0s, 376368 effective words/s\n",
      "2023-01-06 13:41:45,976 : INFO : EPOCH 8: training on 3749 raw words (4004 effective words) took 0.0s, 394297 effective words/s\n",
      "2023-01-06 13:41:45,988 : INFO : EPOCH 9: training on 3749 raw words (3996 effective words) took 0.0s, 374463 effective words/s\n",
      "2023-01-06 13:41:46,000 : INFO : EPOCH 10: training on 3749 raw words (4019 effective words) took 0.0s, 367401 effective words/s\n",
      "2023-01-06 13:41:46,012 : INFO : EPOCH 11: training on 3749 raw words (4008 effective words) took 0.0s, 384255 effective words/s\n",
      "2023-01-06 13:41:46,025 : INFO : EPOCH 12: training on 3749 raw words (4002 effective words) took 0.0s, 373639 effective words/s\n",
      "2023-01-06 13:41:46,037 : INFO : EPOCH 13: training on 3749 raw words (4044 effective words) took 0.0s, 360602 effective words/s\n",
      "2023-01-06 13:41:46,049 : INFO : EPOCH 14: training on 3749 raw words (4016 effective words) took 0.0s, 366680 effective words/s\n",
      "2023-01-06 13:41:46,062 : INFO : EPOCH 15: training on 3749 raw words (4033 effective words) took 0.0s, 367745 effective words/s\n",
      "2023-01-06 13:41:46,074 : INFO : EPOCH 16: training on 3749 raw words (4028 effective words) took 0.0s, 374521 effective words/s\n",
      "2023-01-06 13:41:46,086 : INFO : EPOCH 17: training on 3749 raw words (4014 effective words) took 0.0s, 371827 effective words/s\n",
      "2023-01-06 13:41:46,099 : INFO : EPOCH 18: training on 3749 raw words (4040 effective words) took 0.0s, 371156 effective words/s\n",
      "2023-01-06 13:41:46,110 : INFO : EPOCH 19: training on 3749 raw words (4042 effective words) took 0.0s, 392600 effective words/s\n",
      "2023-01-06 13:41:46,123 : INFO : EPOCH 20: training on 3749 raw words (4017 effective words) took 0.0s, 369912 effective words/s\n",
      "2023-01-06 13:41:46,136 : INFO : EPOCH 21: training on 3749 raw words (4015 effective words) took 0.0s, 358827 effective words/s\n",
      "2023-01-06 13:41:46,148 : INFO : EPOCH 22: training on 3749 raw words (4026 effective words) took 0.0s, 366889 effective words/s\n",
      "2023-01-06 13:41:46,160 : INFO : EPOCH 23: training on 3749 raw words (3997 effective words) took 0.0s, 388550 effective words/s\n",
      "2023-01-06 13:41:46,172 : INFO : EPOCH 24: training on 3749 raw words (4014 effective words) took 0.0s, 372924 effective words/s\n",
      "2023-01-06 13:41:46,184 : INFO : EPOCH 25: training on 3749 raw words (4028 effective words) took 0.0s, 364473 effective words/s\n",
      "2023-01-06 13:41:46,196 : INFO : EPOCH 26: training on 3749 raw words (4038 effective words) took 0.0s, 364635 effective words/s\n",
      "2023-01-06 13:41:46,208 : INFO : EPOCH 27: training on 3749 raw words (4017 effective words) took 0.0s, 384666 effective words/s\n",
      "2023-01-06 13:41:46,221 : INFO : EPOCH 28: training on 3749 raw words (4037 effective words) took 0.0s, 364541 effective words/s\n",
      "2023-01-06 13:41:46,233 : INFO : EPOCH 29: training on 3749 raw words (4029 effective words) took 0.0s, 352945 effective words/s\n",
      "2023-01-06 13:41:46,245 : INFO : EPOCH 30: training on 3749 raw words (4005 effective words) took 0.0s, 384458 effective words/s\n",
      "2023-01-06 13:41:46,258 : INFO : EPOCH 31: training on 3749 raw words (4016 effective words) took 0.0s, 379018 effective words/s\n",
      "2023-01-06 13:41:46,270 : INFO : EPOCH 32: training on 3749 raw words (4027 effective words) took 0.0s, 372603 effective words/s\n",
      "2023-01-06 13:41:46,282 : INFO : EPOCH 33: training on 3749 raw words (4026 effective words) took 0.0s, 377775 effective words/s\n",
      "2023-01-06 13:41:46,294 : INFO : EPOCH 34: training on 3749 raw words (4013 effective words) took 0.0s, 380644 effective words/s\n",
      "2023-01-06 13:41:46,306 : INFO : EPOCH 35: training on 3749 raw words (4019 effective words) took 0.0s, 375889 effective words/s\n",
      "2023-01-06 13:41:46,319 : INFO : EPOCH 36: training on 3749 raw words (4003 effective words) took 0.0s, 361333 effective words/s\n",
      "2023-01-06 13:41:46,330 : INFO : EPOCH 37: training on 3749 raw words (4031 effective words) took 0.0s, 379303 effective words/s\n",
      "2023-01-06 13:41:46,342 : INFO : EPOCH 38: training on 3749 raw words (4029 effective words) took 0.0s, 381382 effective words/s\n",
      "2023-01-06 13:41:46,355 : INFO : EPOCH 39: training on 3749 raw words (4006 effective words) took 0.0s, 373587 effective words/s\n",
      "2023-01-06 13:41:46,355 : INFO : Doc2Vec lifecycle event {'msg': 'training on 149960 raw words (160720 effective words) took 0.5s, 321043 effective words/s', 'datetime': '2023-01-06T13:41:46.355404', 'gensim': '4.3.0', 'python': '3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]', 'platform': 'macOS-11.7-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "model_title.train(documents_train_title, total_examples=model_title.corpus_count, epochs=model_title.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08359868 -0.11832076 -0.30858186 -0.27466866 -0.4148984  -0.1669489\n",
      " -0.04561296  0.23579566 -0.09821264  0.09103481 -0.06887446  0.145755\n",
      "  0.208039    0.04267541 -0.29062927  0.31337136  0.23823555 -0.40322718\n",
      "  0.03623195 -0.18021305  0.49135962  0.22844674  0.37351978 -0.18910986\n",
      "  0.05901324 -0.23063377  0.49833933 -0.3782788  -0.33989197 -0.33011428\n",
      "  0.52409685  0.13391775 -0.3700377   0.36670953 -0.652365   -0.18384698\n",
      "  0.04779305 -0.44073358 -0.29831997 -0.48069853  0.22338131  0.05054268\n",
      "  0.18892121  0.27125472  0.34869316  0.11090005 -0.1925308  -0.52897936\n",
      " -0.18416573 -0.02294459]\n"
     ]
    }
   ],
   "source": [
    "vector = model_desc.infer_vector(['only', 'you', 'can', 'prevent', 'forest', '.', 'fires', 'also', 'you', 'need', 'to', 'learn', '.'])\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04481063  0.13160218  0.13666932  0.02029407  0.04107441  0.08313217\n",
      "  0.29314438  0.15591714 -0.45527855 -0.12219346]\n"
     ]
    }
   ],
   "source": [
    "vector = model_title.infer_vector(['only', 'you', 'can', 'prevent', 'forest'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(documents_train_desc)):\n",
    "    inferred_vector = model_desc.infer_vector(documents_train_desc[doc_id].words)\n",
    "    sims = model_desc.dv.most_similar([inferred_vector], topn=len(model_desc.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1000})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (229): «Batimam and Robim be two close friend . their real name be Léo and Mário , but they 've get those nickname after dress up for mardi - gras as Batman and Robin . to get some money , they decide to rob a small supermarket , but they meet some reaction and Mário gets shoot by the owner , who be kill by Léo . they run away and find solace in an abandon house . Léo decide to go out to try to get some help for his friend , whose leg be wound , but to no avail . when he return , the two start an argument .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
      "\n",
      "MOST (413, 0.6061080098152161): «thirteen year old Audrey Shaw be new to Bellweather , California and certain she will never fit in when she meet Krista Rich , the pretty , popular girl she aspire to be \" or be with \" - she be just not sure . Set against the backdrop of abstinence education and present as Audrey 's real life documentary assemble entirely from her own video clip , instant message and cell phone cam ( as well as those of her classmate and internet friend ) , Sex and the U.S.A. be a script drama about two teenage girl tear between Americas popular culture , where twelve year old bear skin , breast implant be a Sweet Sixteen gift and George W. Bush 's evangelical right prize virginity . where natural urge of adolescence be at well dismiss , at bad , demonize and either way , teach in public school . without sex education or parent who be willing to talk about sex , Audrey and Krista be face with a difficult task : to remain virgin while act like whore . like many american girl , this quest lead they to do almost anything in the name of be popular - everything in fact , except have intercourse . their behavior escalate until ultimately , combine with confusion , result in the forced loss of the only thing that s important to Krista \" her virginity \" , and the only thing that s important to Audrey \" their friendship \" .»\n",
      "\n",
      "MOST2 (28, 0.600651204586029): «EWR 's Revolution 2.0 09/04/2005 1 . Tagteam Reality Check w / Marty Milenko vs Los Tabarnacos 2 . Kona w / Vanessa Kraven vs Fred \" La merveille \" Lauzon w / Académie de lutte du Québec 3 . damian Steele w/ Marty Milenko & Reality Check vs Jack - a - lidster 4 . 2.0 vs Académie de lutte du Québec w / Marc Le Grizzly 5 . Kevin Steen vs Hellstorm 6 . damian vs Pee - Wee 7 . franky \" the Mobster \" vs Homicide 8 . TLC Match ( ewr championship # 1 contendership ) excess vs Jake Matthews w / Lollipop vs \" x - treem \" Dave Silva vs The Arsenal»\n",
      "\n",
      "MEDIAN (470, 0.26760804653167725): «like we , animal be expose to parasite , bacteria and virus - the germ which cause disease . how do they survive these attack ? recent research and observation have show that animal use plant and insect substance to treat themselves - not only do they apply thing to their skin , they actually treat themselves by feed on thing not normally part of their diet . Capuchin monkey rub citrus fruit on their fur , caterpillar eat poison hemlock , herbivorous red deer have even be see chew the leg off live seabird . this film take off around the world to discover how animal use medicine , it question what notion they have about health and how medical knowledge be pass on from one generation to the next . in do so , the film also ask what we can learn from animal about medicine .»\n",
      "\n",
      "LEAST (765, -0.08892909437417984): «use the cover of an Atlanta limousine driver , Jabari Hayes traffic large quantity of cocaine across the country for the then large african american drug organization in the southeast know as Black Mafia Family , often refer to as BMF .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(documents_test_desc) - 1)\n",
    "inferred_vector = model_desc.infer_vector(list(documents_test_desc[doc_id])[0])\n",
    "sims = model_desc.dv.most_similar([inferred_vector], topn=len(model_desc.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(list(documents_test_desc[doc_id])[0])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model_desc)\n",
    "for label, index in [('MOST', 0), ('MOST2', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(documents_test_desc[sims[index][0]].words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preproc['description_num'] = df_train_preproc.apply(lambda x: model_desc.infer_vector(x['description_lem']), axis=1)\n",
    "df_test_preproc['description_num'] = df_test_preproc.apply(lambda x: model_desc.infer_vector(x['description_lem']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preproc['title_num'] = df_train_preproc.apply(lambda x: model_title.infer_vector(x['title_lem']), axis=1)\n",
    "df_test_preproc['title_num'] = df_test_preproc.apply(lambda x: model_title.infer_vector(x['title_lem']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_year(row):\n",
    "    try:\n",
    "        return float(row['year'][0:4])\n",
    "    except:\n",
    "        return float(0)\n",
    "    \n",
    "df_train_preproc['year_num'] = df_train_preproc.apply(numeric_year, axis=1)\n",
    "df_test_preproc['year_num'] = df_test_preproc.apply(numeric_year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>description</th>\n",
       "      <th>action</th>\n",
       "      <th>adult</th>\n",
       "      <th>adventure</th>\n",
       "      <th>animation</th>\n",
       "      <th>biography</th>\n",
       "      <th>comedy</th>\n",
       "      <th>crime</th>\n",
       "      <th>...</th>\n",
       "      <th>sport</th>\n",
       "      <th>talk-show</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "      <th>description_lem</th>\n",
       "      <th>title_lem</th>\n",
       "      <th>description_num</th>\n",
       "      <th>title_num</th>\n",
       "      <th>year_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oscar et la dame rose</td>\n",
       "      <td>2009</td>\n",
       "      <td>Listening in to a conversation between his doc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[listen, in, to, a, conversation, between, his...</td>\n",
       "      <td>[Oscar, et, la, dame, rise]</td>\n",
       "      <td>[0.3168606, 0.73860466, -0.7193371, -0.3311353...</td>\n",
       "      <td>[0.100330554, 0.16899207, 0.34675777, 0.096694...</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cupid</td>\n",
       "      <td>1997</td>\n",
       "      <td>A brother and sister with a past incestuous re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[a, brother, and, sister, with, a, past, inces...</td>\n",
       "      <td>[cupid]</td>\n",
       "      <td>[-0.618494, 0.57840586, -0.3653022, 0.4022861,...</td>\n",
       "      <td>[0.09445099, 0.11505941, 0.13119431, 0.0337071...</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Young, Wild and Wonderful</td>\n",
       "      <td>1980</td>\n",
       "      <td>As the bus empties the students for their fiel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[as, the, bus, empty, the, student, for, their...</td>\n",
       "      <td>[young, ,, wild, and, wonderful]</td>\n",
       "      <td>[-1.0658501, 0.17734432, 1.025234, -1.0661346,...</td>\n",
       "      <td>[0.037205953, 0.12656404, 0.17395878, 0.086817...</td>\n",
       "      <td>1980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Secret Sin</td>\n",
       "      <td>1915</td>\n",
       "      <td>To help their unemployed father make ends meet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[to, help, their, unemployed, father, make, en...</td>\n",
       "      <td>[the, Secret, sin]</td>\n",
       "      <td>[1.4273374, 2.2096736, -0.21864274, 0.13609058...</td>\n",
       "      <td>[0.07300471, 0.057478014, 0.12722696, 0.023133...</td>\n",
       "      <td>1915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Unrecovered</td>\n",
       "      <td>2007</td>\n",
       "      <td>The film's title refers not only to the un-rec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, film, 's, title, refer, not, only, to, t...</td>\n",
       "      <td>[the, Unrecovered]</td>\n",
       "      <td>[-0.43177173, -0.079410814, 0.7588936, -1.1227...</td>\n",
       "      <td>[0.08164266, 0.061981197, 0.06609384, 0.068311...</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  year  \\\n",
       "0      Oscar et la dame rose  2009   \n",
       "1                      Cupid  1997   \n",
       "2  Young, Wild and Wonderful  1980   \n",
       "3             The Secret Sin  1915   \n",
       "4            The Unrecovered  2007   \n",
       "\n",
       "                                         description  action  adult  \\\n",
       "0  Listening in to a conversation between his doc...       0      0   \n",
       "1  A brother and sister with a past incestuous re...       0      0   \n",
       "2  As the bus empties the students for their fiel...       0      1   \n",
       "3  To help their unemployed father make ends meet...       0      0   \n",
       "4  The film's title refers not only to the un-rec...       0      0   \n",
       "\n",
       "   adventure  animation  biography  comedy  crime  ...  sport  talk-show  \\\n",
       "0          0          0          0       0      0  ...      0          0   \n",
       "1          0          0          0       0      0  ...      0          0   \n",
       "2          0          0          0       0      0  ...      0          0   \n",
       "3          0          0          0       0      0  ...      0          0   \n",
       "4          0          0          0       0      0  ...      0          0   \n",
       "\n",
       "   thriller  war  western                                    description_lem  \\\n",
       "0         0    0        0  [listen, in, to, a, conversation, between, his...   \n",
       "1         1    0        0  [a, brother, and, sister, with, a, past, inces...   \n",
       "2         0    0        0  [as, the, bus, empty, the, student, for, their...   \n",
       "3         0    0        0  [to, help, their, unemployed, father, make, en...   \n",
       "4         0    0        0  [the, film, 's, title, refer, not, only, to, t...   \n",
       "\n",
       "                          title_lem  \\\n",
       "0       [Oscar, et, la, dame, rise]   \n",
       "1                           [cupid]   \n",
       "2  [young, ,, wild, and, wonderful]   \n",
       "3                [the, Secret, sin]   \n",
       "4                [the, Unrecovered]   \n",
       "\n",
       "                                     description_num  \\\n",
       "0  [0.3168606, 0.73860466, -0.7193371, -0.3311353...   \n",
       "1  [-0.618494, 0.57840586, -0.3653022, 0.4022861,...   \n",
       "2  [-1.0658501, 0.17734432, 1.025234, -1.0661346,...   \n",
       "3  [1.4273374, 2.2096736, -0.21864274, 0.13609058...   \n",
       "4  [-0.43177173, -0.079410814, 0.7588936, -1.1227...   \n",
       "\n",
       "                                           title_num  year_num  \n",
       "0  [0.100330554, 0.16899207, 0.34675777, 0.096694...    2009.0  \n",
       "1  [0.09445099, 0.11505941, 0.13119431, 0.0337071...    1997.0  \n",
       "2  [0.037205953, 0.12656404, 0.17395878, 0.086817...    1980.0  \n",
       "3  [0.07300471, 0.057478014, 0.12722696, 0.023133...    1915.0  \n",
       "4  [0.08164266, 0.061981197, 0.06609384, 0.068311...    2007.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_preproc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build 2 models and unittest them:\n",
    "    - ready-to-use model from a popular library\n",
    "    - implement LSTM with PyTorch\n",
    "    - implement Doc2Vec with NNs (*)\n",
    "    - implement a model using transformers ^^\n",
    "\n",
    "3. Train & debug models:\n",
    "    - ready-to-use: 0-small # of bugs expected\n",
    "    - LSTM: moderate # of bugs expected\n",
    "    - Transformers: high # of bugs expected\n",
    "\n",
    "4. Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=61, out_features=64, bias=True)\n",
      "  (activ1): ReLU()\n",
      "  (layer2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (activ2): ReLU()\n",
      "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (activ3): ReLU()\n",
      "  (layer4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (activ4): ReLU()\n",
      "  (layer5): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (activ5): ReLU()\n",
      "  (layer6): Linear(in_features=32, out_features=27, bias=True)\n",
      "  (activ6): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Neural Netowrks\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(61, 64)\n",
    "        self.activ1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 128)\n",
    "        self.activ2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.activ3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.activ4 = nn.ReLU()\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.activ5 = nn.ReLU()\n",
    "        self.layer6 = nn.Linear(32, 27)\n",
    "        self.activ6 = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.activ1(self.layer1(x))\n",
    "        x = self.activ2(self.layer2(x))\n",
    "        x = self.activ3(self.layer3(x))\n",
    "        x = self.activ4(self.layer4(x))\n",
    "        x = self.activ5(self.layer5(x))\n",
    "        x = self.activ6(self.layer6(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0296, 0.0420, 0.0298, 0.0425, 0.0399, 0.0369, 0.0393, 0.0385, 0.0301,\n",
       "          0.0314, 0.0336, 0.0394, 0.0328, 0.0348, 0.0384, 0.0428, 0.0424, 0.0316,\n",
       "          0.0363, 0.0409, 0.0412, 0.0338, 0.0390, 0.0369, 0.0402, 0.0413, 0.0343],\n",
       "         [0.0296, 0.0422, 0.0299, 0.0426, 0.0398, 0.0369, 0.0392, 0.0385, 0.0300,\n",
       "          0.0315, 0.0336, 0.0394, 0.0327, 0.0348, 0.0384, 0.0429, 0.0424, 0.0316,\n",
       "          0.0362, 0.0409, 0.0412, 0.0340, 0.0390, 0.0369, 0.0401, 0.0414, 0.0343],\n",
       "         [0.0295, 0.0421, 0.0298, 0.0425, 0.0398, 0.0369, 0.0393, 0.0384, 0.0300,\n",
       "          0.0314, 0.0336, 0.0394, 0.0327, 0.0348, 0.0385, 0.0429, 0.0424, 0.0316,\n",
       "          0.0363, 0.0409, 0.0412, 0.0341, 0.0390, 0.0369, 0.0401, 0.0414, 0.0344],\n",
       "         [0.0295, 0.0422, 0.0298, 0.0426, 0.0398, 0.0370, 0.0393, 0.0385, 0.0300,\n",
       "          0.0315, 0.0336, 0.0393, 0.0327, 0.0348, 0.0384, 0.0428, 0.0423, 0.0316,\n",
       "          0.0363, 0.0409, 0.0412, 0.0341, 0.0389, 0.0369, 0.0402, 0.0413, 0.0344]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor(4., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4, 61)\n",
    "y = net.forward(x)\n",
    "y, torch.sum(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1191, -0.0734,  0.0677,  ..., -0.0490, -0.0699,  0.0940],\n",
      "        [-0.0222,  0.0005,  0.0694,  ..., -0.0671,  0.0346, -0.1096],\n",
      "        [-0.0573,  0.0684,  0.0705,  ...,  0.0504, -0.1097, -0.0275],\n",
      "        ...,\n",
      "        [-0.0507,  0.0517,  0.0286,  ..., -0.0593,  0.1070,  0.0243],\n",
      "        [-0.0912,  0.0592,  0.0491,  ...,  0.0282,  0.0592, -0.0282],\n",
      "        [-0.0817,  0.1048, -0.0535,  ..., -0.0323, -0.0079,  0.0350]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for layer in net.parameters():\n",
    "    print(layer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(dataframe):\n",
    "    numpy_array = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        numpy_array.append([v for v in np.hstack(row)])\n",
    "        \n",
    "    return np.array(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y) -> None:\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "input_features_preproc = ['title_num', 'description_num', 'year_num']\n",
    "X_train = preprocess_dataframe(df_train_preproc[input_features_preproc])\n",
    "y_train = preprocess_dataframe(df_train_preproc[output_features_preproc])\n",
    "X_test = preprocess_dataframe(df_test_preproc[input_features_preproc])\n",
    "y_test = preprocess_dataframe(df_test_preproc[output_features_preproc])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "dataset_train = MyDataset(X_train_scaled, y_train)\n",
    "dataset_test = MyDataset(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(dataset_train, batch_size = 64, shuffle=True)\n",
    "testloader = DataLoader(dataset_test, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.9863,  1.6536,  0.7074, -0.1097,  0.1007,  1.1804,  1.3120,  2.0470,\n",
       "         -1.4191, -2.0231,  0.3889, -0.2735,  0.1379,  0.5786, -0.4416,  2.1631,\n",
       "         -0.6613, -0.5515, -0.1823, -0.3288, -0.2831,  1.4633,  0.0908,  1.2914,\n",
       "         -0.2179, -0.3454, -1.0150, -0.2527,  0.9187, -0.0833,  0.7888, -0.0844,\n",
       "          1.3809,  0.1413,  0.9130,  0.3744,  0.7089,  0.8619,  0.1826,  0.1132,\n",
       "         -0.0904,  0.3597, -0.3438, -1.3123, -0.1977,  0.3000, -0.9360,  0.7821,\n",
       "         -1.1398,  0.1302, -0.4121,  0.1427, -0.3574, -0.8915,  0.2864,  0.1160,\n",
       "         -0.0805, -0.6023, -0.6189,  0.2046,  0.2544], dtype=torch.float64),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0]))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 61]), torch.Size([64, 27]))"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 3.294591471552849\n",
      "Epoch 1 - loss: 3.291183188557625\n",
      "Epoch 2 - loss: 3.2728959023952484\n",
      "Epoch 3 - loss: 3.1912759989500046\n",
      "Epoch 4 - loss: 3.1298499554395676\n",
      "Epoch 5 - loss: 3.1113830357789993\n",
      "Epoch 6 - loss: 3.103951781988144\n",
      "Epoch 7 - loss: 3.0928209722042084\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([6.5951e-04, 1.5937e-02, 1.8563e-04, 9.1542e-04, 1.7343e-03, 5.7758e-03,\n",
      "        2.7016e-04, 6.4914e-01, 2.5943e-01, 3.8451e-04, 1.5999e-03, 1.0658e-04,\n",
      "        4.1865e-03, 2.1503e-02, 3.0244e-03, 1.8397e-03, 9.9605e-05, 2.3898e-03,\n",
      "        6.7627e-03, 7.3271e-04, 1.4461e-03, 7.8234e-03, 6.7100e-03, 9.5654e-04,\n",
      "        5.5625e-03, 6.1749e-04, 2.0053e-04], grad_fn=<SelectBackward0>) tensor([6.5951e-04, 1.5937e-02, 1.8563e-04, 9.1542e-04, 1.7343e-03, 5.7758e-03,\n",
      "        2.7016e-04, 6.4914e-01, 2.5943e-01, 3.8451e-04, 1.5999e-03, 1.0658e-04,\n",
      "        4.1865e-03, 2.1503e-02, 3.0244e-03, 1.8397e-03, 9.9605e-05, 2.3898e-03,\n",
      "        6.7627e-03, 7.3271e-04, 1.4461e-03, 7.8234e-03, 6.7100e-03, 9.5654e-04,\n",
      "        5.5625e-03, 6.1749e-04, 2.0053e-04], grad_fn=<SelectBackward0>) 3.0985660552978516\n",
      "Epoch 8 - loss: 3.083749607205391\n",
      "Epoch 9 - loss: 3.0760674476623535\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.0821e-03, 1.8913e-02, 3.4335e-04, 1.3142e-03, 2.5983e-03, 7.3577e-03,\n",
      "        4.6020e-04, 5.5432e-01, 3.2936e-01, 6.2128e-04, 2.1604e-03, 2.1040e-04,\n",
      "        5.4952e-03, 2.4588e-02, 4.1346e-03, 2.6323e-03, 1.8605e-04, 3.2642e-03,\n",
      "        9.0286e-03, 1.1327e-03, 2.1262e-03, 9.9655e-03, 8.4172e-03, 1.4504e-03,\n",
      "        7.4804e-03, 1.0111e-03, 3.5523e-04], grad_fn=<SelectBackward0>) tensor([1.0821e-03, 1.8913e-02, 3.4335e-04, 1.3142e-03, 2.5983e-03, 7.3577e-03,\n",
      "        4.6020e-04, 5.5432e-01, 3.2936e-01, 6.2128e-04, 2.1604e-03, 2.1040e-04,\n",
      "        5.4952e-03, 2.4588e-02, 4.1346e-03, 2.6323e-03, 1.8605e-04, 3.2642e-03,\n",
      "        9.0286e-03, 1.1327e-03, 2.1262e-03, 9.9655e-03, 8.4172e-03, 1.4504e-03,\n",
      "        7.4804e-03, 1.0111e-03, 3.5523e-04], grad_fn=<SelectBackward0>) 3.0916006565093994\n",
      "Epoch 10 - loss: 3.065619468688965\n",
      "Epoch 11 - loss: 3.0543729662895203\n",
      "Epoch 12 - loss: 3.044560492038727\n",
      "Epoch 13 - loss: 3.0344655364751816\n",
      "Epoch 14 - loss: 3.0266323536634445\n",
      "Epoch 15 - loss: 3.012474462389946\n",
      "Epoch 16 - loss: 3.0068981796503067\n",
      "Epoch 17 - loss: 2.9981207847595215\n",
      "Epoch 18 - loss: 2.9900428354740143\n",
      "Epoch 19 - loss: 2.987048238515854\n",
      "Epoch 20 - loss: 2.9827494621276855\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.5333e-03, 1.4861e-02, 5.7310e-04, 1.3766e-03, 3.4873e-03, 5.5366e-03,\n",
      "        6.4840e-04, 2.6417e-01, 6.2462e-01, 6.8664e-04, 1.8024e-03, 4.2309e-04,\n",
      "        5.2182e-03, 1.9841e-02, 4.1788e-03, 2.9158e-03, 2.7711e-04, 3.0818e-03,\n",
      "        1.0750e-02, 1.3072e-03, 2.3880e-03, 1.0219e-02, 7.0712e-03, 1.6765e-03,\n",
      "        9.2940e-03, 1.4593e-03, 6.0258e-04], grad_fn=<SelectBackward0>) tensor([1.5333e-03, 1.4861e-02, 5.7310e-04, 1.3766e-03, 3.4873e-03, 5.5366e-03,\n",
      "        6.4840e-04, 2.6417e-01, 6.2462e-01, 6.8664e-04, 1.8024e-03, 4.2309e-04,\n",
      "        5.2182e-03, 1.9841e-02, 4.1788e-03, 2.9158e-03, 2.7711e-04, 3.0818e-03,\n",
      "        1.0750e-02, 1.3072e-03, 2.3880e-03, 1.0219e-02, 7.0712e-03, 1.6765e-03,\n",
      "        9.2940e-03, 1.4593e-03, 6.0258e-04], grad_fn=<SelectBackward0>) 2.99298357963562\n",
      "Epoch 21 - loss: 2.97537399828434\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0.]) tensor([2.2860e-08, 3.5431e-05, 1.0590e-09, 5.2505e-08, 2.0809e-07, 6.9158e-06,\n",
      "        3.2572e-09, 7.8962e-01, 2.1022e-01, 5.8502e-09, 1.6318e-07, 4.3200e-10,\n",
      "        1.6729e-06, 9.2513e-05, 9.9297e-07, 3.5191e-07, 3.1138e-10, 5.2381e-07,\n",
      "        4.8014e-06, 3.0828e-08, 1.2094e-07, 8.6984e-06, 4.0952e-06, 5.9083e-08,\n",
      "        4.0598e-06, 2.2869e-08, 2.0261e-09], grad_fn=<SelectBackward0>) tensor([2.2860e-08, 3.5431e-05, 1.0590e-09, 5.2505e-08, 2.0809e-07, 6.9158e-06,\n",
      "        3.2572e-09, 7.8962e-01, 2.1022e-01, 5.8502e-09, 1.6318e-07, 4.3200e-10,\n",
      "        1.6729e-06, 9.2513e-05, 9.9297e-07, 3.5191e-07, 3.1138e-10, 5.2381e-07,\n",
      "        4.8014e-06, 3.0828e-08, 1.2094e-07, 8.6984e-06, 4.0952e-06, 5.9083e-08,\n",
      "        4.0598e-06, 2.2869e-08, 2.0261e-09], grad_fn=<SelectBackward0>) 2.9108543395996094\n",
      "Epoch 22 - loss: 2.9732864052057266\n",
      "Epoch 23 - loss: 2.9674184918403625\n",
      "Epoch 24 - loss: 2.9661058336496353\n",
      "Epoch 25 - loss: 2.9627053290605545\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.5948e-05, 1.4657e-03, 2.5583e-06, 2.4926e-05, 6.2961e-05, 6.0256e-04,\n",
      "        4.6399e-06, 7.5368e-01, 2.3917e-01, 7.7153e-06, 6.2350e-05, 1.2744e-06,\n",
      "        2.3553e-04, 2.5492e-03, 1.4724e-04, 7.8088e-05, 1.1625e-06, 1.0521e-04,\n",
      "        3.6548e-04, 1.9569e-05, 4.7544e-05, 5.9363e-04, 4.0439e-04, 2.8220e-05,\n",
      "        3.1221e-04, 1.4015e-05, 3.3351e-06], grad_fn=<SelectBackward0>) tensor([1.5948e-05, 1.4657e-03, 2.5583e-06, 2.4926e-05, 6.2961e-05, 6.0256e-04,\n",
      "        4.6399e-06, 7.5368e-01, 2.3917e-01, 7.7153e-06, 6.2350e-05, 1.2744e-06,\n",
      "        2.3553e-04, 2.5492e-03, 1.4724e-04, 7.8088e-05, 1.1625e-06, 1.0521e-04,\n",
      "        3.6548e-04, 1.9569e-05, 4.7544e-05, 5.9363e-04, 4.0439e-04, 2.8220e-05,\n",
      "        3.1221e-04, 1.4015e-05, 3.3351e-06], grad_fn=<SelectBackward0>) 2.8841733932495117\n",
      "Epoch 26 - loss: 2.959785833954811\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.4455e-11, 4.2948e-07, 1.9276e-13, 5.5872e-11, 2.9973e-10, 7.0256e-08,\n",
      "        1.0455e-12, 9.3549e-01, 6.4510e-02, 2.6107e-12, 3.0725e-10, 5.5668e-14,\n",
      "        6.6873e-09, 1.7355e-06, 3.1556e-09, 7.1404e-10, 4.0786e-14, 1.3576e-09,\n",
      "        2.1300e-08, 2.6306e-11, 1.6241e-10, 6.2109e-08, 2.2028e-08, 5.8519e-11,\n",
      "        1.8035e-08, 1.4263e-11, 5.2221e-13], grad_fn=<SelectBackward0>) tensor([1.4455e-11, 4.2948e-07, 1.9276e-13, 5.5872e-11, 2.9973e-10, 7.0256e-08,\n",
      "        1.0455e-12, 9.3549e-01, 6.4510e-02, 2.6107e-12, 3.0725e-10, 5.5668e-14,\n",
      "        6.6873e-09, 1.7355e-06, 3.1556e-09, 7.1404e-10, 4.0786e-14, 1.3576e-09,\n",
      "        2.1300e-08, 2.6306e-11, 1.6241e-10, 6.2109e-08, 2.2028e-08, 5.8519e-11,\n",
      "        1.8035e-08, 1.4263e-11, 5.2221e-13], grad_fn=<SelectBackward0>) 3.0246918201446533\n",
      "Epoch 27 - loss: 2.9589942544698715\n",
      "Epoch 28 - loss: 2.9570384323596954\n",
      "Epoch 29 - loss: 2.9542279839515686\n",
      "Epoch 30 - loss: 2.955157071352005\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.1854e-04, 1.5176e-03, 3.6246e-05, 7.8920e-05, 3.6911e-04, 3.0022e-04,\n",
      "        3.6015e-05, 3.8279e-02, 9.4919e-01, 2.5772e-05, 6.7605e-05, 3.1213e-05,\n",
      "        4.1232e-04, 2.5328e-03, 3.8071e-04, 2.5971e-04, 1.1262e-05, 2.1388e-04,\n",
      "        1.8841e-03, 7.1839e-05, 1.6254e-04, 1.4002e-03, 5.7178e-04, 1.1030e-04,\n",
      "        1.7657e-03, 1.2924e-04, 4.4984e-05], grad_fn=<SelectBackward0>) tensor([1.1854e-04, 1.5176e-03, 3.6246e-05, 7.8920e-05, 3.6911e-04, 3.0022e-04,\n",
      "        3.6015e-05, 3.8279e-02, 9.4919e-01, 2.5772e-05, 6.7605e-05, 3.1213e-05,\n",
      "        4.1232e-04, 2.5328e-03, 3.8071e-04, 2.5971e-04, 1.1262e-05, 2.1388e-04,\n",
      "        1.8841e-03, 7.1839e-05, 1.6254e-04, 1.4002e-03, 5.7178e-04, 1.1030e-04,\n",
      "        1.7657e-03, 1.2924e-04, 4.4984e-05], grad_fn=<SelectBackward0>) 2.997251510620117\n",
      "Epoch 31 - loss: 2.952057793736458\n",
      "Epoch 32 - loss: 2.9475731551647186\n",
      "Epoch 33 - loss: 2.9484128654003143\n",
      "Epoch 34 - loss: 2.946606785058975\n",
      "Epoch 35 - loss: 2.946166828274727\n",
      "Epoch 36 - loss: 2.946649059653282\n",
      "Epoch 37 - loss: 2.9450720995664597\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([2.6833e-06, 1.9281e-04, 3.9926e-07, 2.3813e-06, 1.3031e-05, 3.2794e-05,\n",
      "        5.4684e-07, 6.5501e-02, 9.3333e-01, 4.5095e-07, 2.7185e-06, 2.7686e-07,\n",
      "        2.6129e-05, 4.0766e-04, 2.1167e-05, 1.1851e-05, 9.7579e-08, 1.0599e-05,\n",
      "        1.3204e-04, 1.8364e-06, 5.4041e-06, 1.3069e-04, 4.4143e-05, 3.2830e-06,\n",
      "        1.2777e-04, 2.9027e-06, 6.3165e-07], grad_fn=<SelectBackward0>) tensor([2.6833e-06, 1.9281e-04, 3.9926e-07, 2.3813e-06, 1.3031e-05, 3.2794e-05,\n",
      "        5.4684e-07, 6.5501e-02, 9.3333e-01, 4.5095e-07, 2.7185e-06, 2.7686e-07,\n",
      "        2.6129e-05, 4.0766e-04, 2.1167e-05, 1.1851e-05, 9.7579e-08, 1.0599e-05,\n",
      "        1.3204e-04, 1.8364e-06, 5.4041e-06, 1.3069e-04, 4.4143e-05, 3.2830e-06,\n",
      "        1.2777e-04, 2.9027e-06, 6.3165e-07], grad_fn=<SelectBackward0>) 2.935245990753174\n",
      "Epoch 38 - loss: 2.943780481815338\n",
      "Epoch 39 - loss: 2.9443851709365845\n",
      "Epoch 40 - loss: 2.9378871470689774\n",
      "Epoch 41 - loss: 2.9392065703868866\n",
      "Epoch 42 - loss: 2.9402068853378296\n",
      "Epoch 43 - loss: 2.9393829703330994\n",
      "Epoch 44 - loss: 2.9386280924081802\n",
      "Epoch 45 - loss: 2.938066080212593\n",
      "Epoch 46 - loss: 2.9374750703573227\n",
      "Epoch 47 - loss: 2.938757672905922\n",
      "Epoch 48 - loss: 2.9326681345701218\n",
      "Epoch 49 - loss: 2.9382447004318237\n",
      "Epoch 50 - loss: 2.932496026158333\n",
      "Epoch 51 - loss: 2.9332163631916046\n",
      "Epoch 52 - loss: 2.9349439591169357\n",
      "Epoch 53 - loss: 2.9324394166469574\n",
      "Epoch 54 - loss: 2.9316218495368958\n",
      "Epoch 55 - loss: 2.933035910129547\n",
      "Epoch 56 - loss: 2.931274488568306\n",
      "Epoch 57 - loss: 2.9302745312452316\n",
      "Epoch 58 - loss: 2.929055407643318\n",
      "Epoch 59 - loss: 2.9304179251194\n",
      "Epoch 60 - loss: 2.925800785422325\n",
      "Epoch 61 - loss: 2.9260387420654297\n",
      "Epoch 62 - loss: 2.928904116153717\n",
      "Epoch 63 - loss: 2.92923304438591\n",
      "Epoch 64 - loss: 2.92849825322628\n",
      "Epoch 65 - loss: 2.930921271443367\n",
      "Epoch 66 - loss: 2.9291120022535324\n",
      "Epoch 67 - loss: 2.924640715122223\n",
      "Epoch 68 - loss: 2.9281913489103317\n",
      "Epoch 69 - loss: 2.9254682660102844\n",
      "Epoch 70 - loss: 2.9290854781866074\n",
      "Epoch 71 - loss: 2.927384927868843\n",
      "Epoch 72 - loss: 2.927194058895111\n",
      "Epoch 73 - loss: 2.927159398794174\n",
      "Epoch 74 - loss: 2.925472930073738\n",
      "Epoch 75 - loss: 2.9259528666734695\n",
      "Epoch 76 - loss: 2.9267753213644028\n",
      "Epoch 77 - loss: 2.9254381507635117\n",
      "Epoch 78 - loss: 2.9234150797128677\n",
      "Epoch 79 - loss: 2.926761269569397\n",
      "Epoch 80 - loss: 2.9222662895917892\n",
      "Epoch 81 - loss: 2.9221302568912506\n",
      "Epoch 82 - loss: 2.9184484630823135\n",
      "Epoch 83 - loss: 2.9221276342868805\n",
      "Epoch 84 - loss: 2.9264524281024933\n",
      "Epoch 85 - loss: 2.9193729013204575\n",
      "Epoch 86 - loss: 2.9220216423273087\n",
      "Epoch 87 - loss: 2.921703204512596\n",
      "Epoch 88 - loss: 2.9189306497573853\n",
      "Epoch 89 - loss: 2.9199362695217133\n",
      "Epoch 90 - loss: 2.919650077819824\n",
      "Epoch 91 - loss: 2.9204413890838623\n",
      "Epoch 92 - loss: 2.9193396866321564\n",
      "Epoch 93 - loss: 2.918164223432541\n",
      "Epoch 94 - loss: 2.9190120846033096\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([2.9255e-08, 2.2206e-05, 1.7703e-09, 4.3939e-08, 2.5454e-07, 5.3180e-06,\n",
      "        3.9480e-09, 3.6976e-01, 6.3013e-01, 4.9583e-09, 1.0327e-07, 6.9344e-10,\n",
      "        1.1525e-06, 5.8392e-05, 6.7133e-07, 3.5975e-07, 4.1728e-10, 3.4683e-07,\n",
      "        3.6101e-06, 2.6933e-08, 1.1101e-07, 1.0694e-05, 2.6671e-06, 5.1870e-08,\n",
      "        4.2114e-06, 2.4658e-08, 3.7074e-09], grad_fn=<SelectBackward0>) tensor([2.9255e-08, 2.2206e-05, 1.7703e-09, 4.3939e-08, 2.5454e-07, 5.3180e-06,\n",
      "        3.9480e-09, 3.6976e-01, 6.3013e-01, 4.9583e-09, 1.0327e-07, 6.9344e-10,\n",
      "        1.1525e-06, 5.8392e-05, 6.7133e-07, 3.5975e-07, 4.1728e-10, 3.4683e-07,\n",
      "        3.6101e-06, 2.6933e-08, 1.1101e-07, 1.0694e-05, 2.6671e-06, 5.1870e-08,\n",
      "        4.2114e-06, 2.4658e-08, 3.7074e-09], grad_fn=<SelectBackward0>) 2.987189531326294\n",
      "Epoch 95 - loss: 2.919087663292885\n",
      "Epoch 96 - loss: 2.9196696132421494\n",
      "Epoch 97 - loss: 2.9204968959093094\n",
      "Epoch 98 - loss: 2.9179864823818207\n",
      "Epoch 99 - loss: 2.9154145568609238\n"
     ]
    }
   ],
   "source": [
    "def train(model, loss_fn, dataloader, optimizer, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_p = model(X.float())\n",
    "\n",
    "            loss = loss_fn(y_p.float(), y.float())\n",
    "\n",
    "            if np.random.uniform(0, 1) < 0.01:\n",
    "                print(y[0].float(), y_p[0].float(), model(X.float())[0], loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} - loss: {epoch_loss/len(dataloader)}\")\n",
    "\n",
    "net = Net()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(params=net.parameters(), lr = 1e-3, lr_decay = 1e-5, weight_decay=1e-5)\n",
    "\n",
    "train(net, loss_fn, trainloader, optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4437)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [0, 1, 0, 0]\n",
    "w = [0.3, 0.3, 0.2, 0.2]\n",
    "x = torch.tensor(v).float()\n",
    "y = torch.tensor(w).float()\n",
    "nn.CrossEntropyLoss()(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.5704, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('deepsenseai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb7720597c668dab0846ca2cc640fd001ce837d43a9c71f81a265cbfaca086e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
